{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='../img/clase14/Neurona.png' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "### Redes Neuronales y Transformers\n",
    "#### NLP - Anal√≠tica Estrat√©gica de Datos\n",
    "<br><b>Fundaci√≥n Universitaria Konrad Lorenz</b>\n",
    "<br>Docente: Viviana M√°rquez [vivianam.penama@konradlorenz.edu.co](mailto:vivianama.penam@konradlorenz.edu.co)\n",
    "<br>Clase #14: Mayo 27, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Retroalimentaci√≥n taller 11 y taller 12 \n",
    "(El taller 9 a√∫n est√° pendiente por calificar, les entrego notas la pr√≥xima clase de lo que tenemos...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üöÄ Hoy veremos...\n",
    "\n",
    "- ¬øQu√© es NLP? (Recorderis)\n",
    "- Evoluci√≥n de NLP\n",
    "- Visi√≥n general sobre las redes neuronales\n",
    "- RNN (Red neuronal recurrente) para generar texto\n",
    "- Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¬øQu√© es NLP?\n",
    "\n",
    "<center><img src='../img/clase14/vector.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- El **Procesamiento de Lenguaje Natural** es una √°rea de la Inteligencia Artificial que permite a los computadores entender, interpretar y usar el lenguaje humano. Es una herramienta que nos permite obtener informaci√≥n a partir del lenguaje humano (normalmente textos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evoluci√≥n de NLP\n",
    "\n",
    "<center><img src='../img/clase14/evol.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- 1950: Alan Turing describi√≥ una \"m√°quina pensante\". Afirm√≥ que si una m√°quina pudiera participar en una conversaci√≥n e imitara a un humano de forma tan completa que no hubiera diferencias perceptibles, entonces la m√°quina podr√≠a considerarse capaz de pensar. <br><br>\n",
    "\n",
    "- 1952: El modelo Hodgkin-Huxley mostr√≥ c√≥mo el cerebro utiliza las neuronas para formar una red el√©ctrica. <br><br>\n",
    "\n",
    "- ^ Eso inspir√≥ la creaci√≥n de AI & NLP & la evoluci√≥n de los computadores. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evoluci√≥n de NLP\n",
    "\n",
    "<center><img src='../img/clase14/evol.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- 1954: **Bag of Words**\n",
    "\n",
    "    - Cuenta la ocurrencia de palabras en un documento.\n",
    "    - D√≠ficil de aplicar en la vida real porque r√°pidamente ocupa mucha memoria y es dominado por las palabras vac√≠as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evoluci√≥n de NLP\n",
    "\n",
    "<center><img src='../img/clase14/evol.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- 1972: **TF-IDF**\n",
    "\n",
    "    - Elimin√≥ el problema de las palabras vac√≠as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evoluci√≥n de NLP\n",
    "\n",
    "<center><img src='../img/clase14/evol.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- 2013: **Word2Vec**\n",
    "\n",
    "    - T√©cnica que toma encuenta la relaci√≥n sem√°ntica entre palabras\n",
    "    - De los primeros modelos que usaron t√©cnicas de predicci√≥n y redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evoluci√≥n de NLP\n",
    "\n",
    "<center><img src='../img/clase14/evol.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- 2018: **Transformers**\n",
    "\n",
    "    - Modelos que usan atenci√≥n para aumentar la velocidad del entrenamiento en tareas espec√≠ficas. \n",
    "    \n",
    "**Recordemos** que NLP tiene muchas aplicaciones en la vida real que usan diferentes t√©cnicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üß† Redes Neuronales y NLP\n",
    "\n",
    "<center><img src='../img/clase14/evol_rn.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üß† Redes Neuronales\n",
    "\n",
    "YouTube: https://www.youtube.com/watch?v=cQ54GDm1eL0&ab_channel=BuzzFeedVideo\n",
    "\n",
    "- Una red neuronal est√° compuesta de neuronas\n",
    "- Las **Redes Neuronales Artifciales** (ANN) est√°n inspiradas las redes neuronales biol√≥gicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üß† Una neurona biol√≥gica\n",
    "\n",
    "<center><img src='../img/clase14/neu.png' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "De una manera simplificada:\n",
    "- Las dendritas alimentan el cuerpo de la c√©lula a trav√©s de se√±ales el√©ctricas\n",
    "- La respuesta es despu√©s pasada a trav√©s del ax√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üß† Una neurona artificial = Perceptr√≥n \n",
    "\n",
    "<center><img src='../img/clase14/perceptron.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üß† Una neurona artificial = Perceptr√≥n \n",
    "\n",
    "<center><img src='../img/clase14/p1.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Como todo modelo de *machine learning*, tenemos valores de entrada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üß† Una neurona artificial = Perceptr√≥n \n",
    "\n",
    "<center><img src='../img/clase14/p2b.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Despu√©s, los valores de entrada son multiplicados por pesos\n",
    "- Estos pesos son inicializados de manera aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üß† Una neurona artificial = Perceptr√≥n \n",
    "\n",
    "<center><img src='../img/clase14/p3.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- El resultado (en este ejemplo $12\\cdot.5 + 4\\cdot-1 = 2$) se pasa por una funci√≥n de activaci√≥n\n",
    "- Existen muchas funciones de activaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üß† Una neurona artificial = Perceptr√≥n \n",
    "\n",
    "<center><img src='../img/clase14/p4.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- El resultado (en este ejemplo $12\\cdot.5 + 4\\cdot-1 = 2$) se pasa por una funci√≥n de activaci√≥n\n",
    "- Existen muchas funciones de activaci√≥n: **EJEMPLO**: Positiva=1, Negativa=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üß† Una neurona artificial = Perceptr√≥n \n",
    "<br>\n",
    "<center><img src='../img/clase14/p5.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Se agrega un sesgo para evitar problemas matem√°ticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üß† Una red neuronal artificial\n",
    "<br>\n",
    "<center><img src='../img/clase14/p5.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Matem√°ticamente tenemos:\n",
    "$$\\sum^{n}_{i=0}w_ix_i + b$$\n",
    "<br>\n",
    "- Cuando tenemos una red neuronal compuesta de varios perceptrones, se extiende a forma matricial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üß† Una red neuronal artificial\n",
    "<br>\n",
    "<center><img src='../img/clase14/ann.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "Partes:\n",
    "- Capa de entrada (input): Valores reales de los datos\n",
    "- Capas ocultas (2 en este caso): Deep network\n",
    "- Capa de salida (output): Estimado final\n",
    "\n",
    "A medida que crece el n√∫mero de capas, crece el nivel de abstracci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üëÆ‚Äç‚ôÄÔ∏è Punto de control\n",
    "<br>\n",
    "<center><img src='../img/clase14/control.png' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "### ¬øCu√°ntas capas ocultas tiene esta red neuronal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üí• Funci√≥n de activaci√≥n\n",
    "<br>\n",
    "<center><img src='../img/clase14/fa1.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Asigna 0 si el valor es menor a 0, asigna 1 si el valor es mayor o igual a cero\n",
    "- Muy dr√°stica, peque√±os cambios no son reflejados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üí• Funci√≥n de activaci√≥n\n",
    "<br>\n",
    "<center><img src='../img/clase14/fa2.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- **Funci√≥n sigmoide:** $f(x)=\\dfrac{1}{1+e^{-(x)}}$\n",
    "- Muy √∫til dependiendo de la tarea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üí• Funci√≥n de activaci√≥n\n",
    "<br>\n",
    "<center><img src='../img/clase14/fa3.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- **Funci√≥n de la tangente hiperb√≥lica:** $\\tanh x=\\dfrac{\\sinh x}{\\cosh x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üí• Funci√≥n de activaci√≥n\n",
    "<br>\n",
    "<center><img src='../img/clase14/fa4.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- **Funci√≥n ReLU** (Rectificador lineal): $max(0,x)$\n",
    "- Aunque es sencilla, tiene muy buen desempe√±o en muchas situaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üß† RNN: Redes Neuronales Recurrentes\n",
    "\n",
    "- Dise√±adas para trabajar con datos de **secuencia **\n",
    "    - Series de tiempo\n",
    "    - Frases\n",
    "    - Audio\n",
    "    - Trayector√≠a de carros\n",
    "    - M√∫sica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üß† RNN: Redes Neuronales Recurrentes\n",
    "<br>\n",
    "<center><img src='../img/clase14/rnnvsann.png' style='height:450px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Se env√≠a el resultado a s√≠ misma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ü§î RNN: Redes Neuronales Recurrentes\n",
    "\n",
    "- Un problema de las RNN es que empiezan a olvidar los primeros datos de entrada\n",
    "- Soluciones: \n",
    "    - LSTM (Long Short-Term Memory) \n",
    "    - GRU (Gated Recurrent Unit)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notebook de Redes Neuronales\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ü§ó Transformers\n",
    "<br>\n",
    "<center><img src='../img/clase14/trans.jpeg' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "\n",
    "## <i> \"Atenci√≥n es todo lo que necesitas\"</i>\n",
    "\n",
    "- Una nueva arquitectura que intenta resolver las tareas de **secuencia**, pero sin usar RNN.\n",
    "- En cambio utilizan un mecanismo de \"atenci√≥n\" que hace peso en las distintas partes de los datos de entrada.\n",
    "- Principalmente utilizado en NLP\n",
    "- El mayor problema de los modelos Transformers es el poder computacional que requieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ü§ó Transformers\n",
    "<br>\n",
    "<center><img src='../img/clase14/trans.jpeg' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- [Hugging Face](https://huggingface.co/) es una startup que ofrece 30 modulos pre-entrenados en m√°s de 100 idiomas y 8 arquitecturas para NLU & NLG.\n",
    "\n",
    "    - BERT (from Google);\n",
    "    - GPT (from OpenAI);\n",
    "    - GPT-2 (from OpenAI);\n",
    "    - Transformer-XL (from Google/CMU);\n",
    "    - XLNet (from Google/CMU);\n",
    "    - XLM (from Facebook);\n",
    "    - RoBERTa (from Facebook);\n",
    "    - DistilBERT (from Hugging Face).\n",
    "    \n",
    "    \n",
    "- Recurso: https://www.kdnuggets.com/2021/02/hugging-face-transformer-basics.html\n",
    "- Recurso: https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Predecir texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "resultado = generator(\"Hi, how are you?\", max_length=60, num_return_sequences=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, how are you? And how do you keep the whole system going in just a few days? Thanks so much! Please let us know what you think.\n",
      "\n",
      "Please, please, give us more info about it..\n",
      "\n",
      "If you want, please email us.\n",
      "\n",
      "Till\n",
      "****************************************************************************************************\n",
      "Hi, how are you?\n",
      "\n",
      "\n",
      "Your response on the second page of the blog?\n",
      "\n",
      "\n",
      "Are you still an employee?\n",
      "\n",
      "\n",
      "Yes! I've been a bit of a 'wonderboy' here but just about everybody in the company has worked for me lately and I appreciate that.\n",
      "****************************************************************************************************\n",
      "Hi, how are you? The only way is if she'll come out at this point. Right.\n",
      "\n",
      "Q:\n",
      "\n",
      "Now that we have the best candidate to get this thing going, so what's next? I thought I could tell you a quick summary of that, and then we\n",
      "****************************************************************************************************\n",
      "Hi, how are you? How do you deal with the problem of a bunch of people on this planet that come together, come to live their lives all to themselves, be true to their values.\n",
      "\n",
      "AMY GOODMAN: Talk a little bit about the history of immigration, what's different recently,\n",
      "****************************************************************************************************\n",
      "Hi, how are you?\n",
      "\n",
      "It's very bad, but it's also very good as a good news story. I'm going to tell you an even better story next. I'm about to get out of the hospital and you're here.\n",
      "\n",
      "\n",
      "Have you heard what people have been\n",
      "****************************************************************************************************\n",
      "Hi, how are you? I hope you don't mind. It just won't give me any headaches this summer. I mean, it's all just not happening. So, you've got to be really patient with this, okay?\"\n",
      "\n",
      "\"Yes.\" Harry answered, and then nodded.\n",
      "****************************************************************************************************\n",
      "Hi, how are you? Are you feeling well, my dear? What kind of person are you? You see, the other people in the world have these strange beliefs? And they are always in pain, always. They believe all sorts of nonsense. No, they're very intelligent, they know\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for r in resultado:\n",
    "    print(r['generated_text'])\n",
    "    print(\"*\"*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An√°lisis de sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998819828033447}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('I am so happy with my students')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Respuesta de preguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9964471459388733, 'start': 0, 'end': 6, 'answer': 'Bogot√°'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline('question-answering')\n",
    "question_answerer({\n",
    "    'question': 'What is the capital of Colombia?',\n",
    "    'context': 'Bogot√° is the capital of Colombia'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Predecir texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Hello, my name is David. [SEP]',\n",
       "  'score': 0.007386402226984501,\n",
       "  'token': 1681,\n",
       "  'token_str': 'David'},\n",
       " {'sequence': '[CLS] Hello, my name is James. [SEP]',\n",
       "  'score': 0.006773959379643202,\n",
       "  'token': 1600,\n",
       "  'token_str': 'James'},\n",
       " {'sequence': '[CLS] Hello, my name is Sam. [SEP]',\n",
       "  'score': 0.006729594897478819,\n",
       "  'token': 2687,\n",
       "  'token_str': 'Sam'},\n",
       " {'sequence': '[CLS] Hello, my name is Charlie. [SEP]',\n",
       "  'score': 0.0059125544503331184,\n",
       "  'token': 4117,\n",
       "  'token_str': 'Charlie'},\n",
       " {'sequence': '[CLS] Hello, my name is Kate. [SEP]',\n",
       "  'score': 0.0057940054684877396,\n",
       "  'token': 5036,\n",
       "  'token_str': 'Kate'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='bert-base-cased')\n",
    "unmasker(\"Hello, my name is [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Apollo program, also known as Project Apollo, was the third U.S. human spaceflight program . The first manned flight of Apollo was in 1968 . It was followed by the two-man Project Gemini (1962-66) which ran concurrently with it .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "ARTICLE = \"\"\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972.\n",
    "First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space,\n",
    "Apollo was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress.\n",
    "Project Mercury was followed by the two-man Project Gemini (1962-66).\n",
    "The first manned flight of Apollo was in 1968.\n",
    "Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966.\n",
    "Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions.\n",
    "Apollo used Saturn family rockets as launch vehicles.\n",
    "Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\n",
    "\"\"\"\n",
    "\n",
    "summary=summarizer(ARTICLE, max_length=100, min_length=30, do_sample=False)[0]\n",
    "\n",
    "print(summary['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NER: Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'Ronald',\n",
       "  'score': 0.9978647828102112,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 1},\n",
       " {'word': '##o', 'score': 0.99903804063797, 'entity': 'I-PER', 'index': 2},\n",
       " {'word': 'Juventus',\n",
       "  'score': 0.9977495670318604,\n",
       "  'entity': 'I-ORG',\n",
       "  'index': 11},\n",
       " {'word': 'Portugal',\n",
       "  'score': 0.9991246461868286,\n",
       "  'entity': 'I-LOC',\n",
       "  'index': 13}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "nlp_token_class = pipeline('ner')\n",
    "nlp_token_class('Ronaldo was born in 1985, he plays for Juventus and Portugal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ‚è≥ Hoy vimos\n",
    "\n",
    "\n",
    "- ¬øQu√© es NLP? (Recorderis)\n",
    "- Evoluci√≥n de NLP\n",
    "- Visi√≥n general sobre las redes neuronales\n",
    "- RNN (Red neuronal recurrente) para generar texto\n",
    "- Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¬°Tiempo de taller!\n",
    "\n",
    "<center>\n",
    "<img src='../img/Taller.gif'>\n",
    "</center>  \n",
    "    \n",
    "# ¬°Mentira, ya no hay taller! ü•≥ La pr√≥xima clase entrego notas de lo que tenemos hasta el momento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Portafolio\n",
    "    \n",
    "    \n",
    "- üöß ‚úã 6% Portafolio [Fecha de entrega: Junio 10]\n",
    "    - [Ejemplo](https://xiomydiazanalyst.github.io/Portafolio/)\n",
    "    - Repositorio con lista de enlaces a los talleres y su peque√±a descripci√≥n \n",
    "    - Voy a revisar que todos los talleres est√©n \n",
    "    - Subir link al aula virtual\n",
    "    \n",
    "    \n",
    "# Proyecto Final\n",
    "- üöß ‚úã 16% Proyecto Final [Fecha de entrega: Junio 3]\n",
    "    - [FORMATO](https://docs.google.com/spreadsheets/d/1Ern0oU03LmfpdXzx3cf-oBIRc6G7evEV0MFVUR1aw34/edit?usp=sharing)\n",
    "    - T√≠tulo, peque√±a descripci√≥n del proyecto y los autores\n",
    "    - Exposici√≥n de 10 minutos por grupo\n",
    "        - ¬øCu√°l es el objetivo?\n",
    "        - ¬øC√≥mo consiguieron los datos?\n",
    "        - Problemas que tuvieron\n",
    "        - Resultados\n",
    "    - Github con c√≥digo\n",
    "        - Pre-procesamiento del texto\n",
    "        - Modelo de NLP\n",
    "        - Visualizaci√≥n\n",
    "    - Me gustar√≠a que prendieran la c√°mara para que nos conozcamos :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src='../img/fin.png' style='height:300px; float: center; margin: 0px 15px 15px 0px'>\n",
    "\n",
    "### Pr√≥ximas clases: Proyectos Finales"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
